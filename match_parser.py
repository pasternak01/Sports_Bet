import json

import requests
from bs4 import BeautifulSoup
import datetime

import re

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
    "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
}

year, month, day = datetime.date.today().isoformat().split("-")
url_list = [f"https://www.tennisexplorer.com/madrid/{year}/atp-men/",
       f"https://www.tennisexplorer.com/madrid-wta/{year}/wta-women/"]


def get_soup(url: str, headers: dict):
    """–ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f'Request error: {e}')
        exit()
    soup = BeautifulSoup(response.content, "html.parser")
    return soup


def get_player_info(url: str, headers: dict):
    """–ø–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–µ –∏–º–µ–Ω–∞ –∏–≥—Ä–æ–∫–æ–≤ –∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Ö –ø—Ä–æ—Ñ–∏–ª–∏ –∏–∑ –ø–æ–ª—É—á–µ–Ω–Ω–æ–π —Å—Å—ã–ª–∫–∏ —Ç–µ–Ω–Ω–∏—Å–Ω–æ–≥–æ –º–∞—Ç—á–∞"""

    players_list = []
    soup = get_soup(url, headers=headers)
    players = soup.find_all("th", class_="plName")
    for player in players:
        name = player.find("a").text
        url = f'https://www.tennisexplorer.com{player.find("a")["href"]}'

        d_players = {
            "name": name,
            "url": url
        }
        players_list.append(d_players)
    return players_list[0], players_list[1]


def get_matches_today(url: str, headers: dict) -> list:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∫–æ—Ç–æ—Ä–∞—è —Å–æ–±–∏—Ä–∞–µ—Ç –∏–∑ —Ç–∞–±–ª–∏—Ü—ã –º–∞—Ç—á–∏ –Ω–∞ —Å–µ–≥–æ–¥–Ω—è.
    –í –Ω–µ–π –ø–æ–ª—É—á–∞–µ–º:
    :arg url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    :arg headers: Headers
    :var match_name: —Å–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–µ –∏–º–µ–Ω–∞ –∏–≥—Ä–æ–∫–æ–≤ –∏ –µ—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è –º–∞—Ç—á–∞
    :var match_url: —Å—Å—ã–ª–∫–∞ –Ω–∞ —Å–∞–º –º–∞—Ç—á
    :var match_time: - –≤—Ä–µ–º—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –º–∞—Ç—á–∞
    :var p1 –∏ p2 (player1 –∏ 2) - –∏–≥—Ä–æ–∫–∏ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–∞–º–æ–≥–æ –º–∞—Ç—á–∞ –∏—Ö –ø–æ–ª–Ω—ã–µ –∏–º–µ–Ω–∞ –∏ —Å—Å—ã–ª–∫–∏
    """
    soup = get_soup(url, headers=headers)

    today_table = soup.find("div", id="tournamentTabs-1-data").find_all("tr")

    today_matches = []

    for row in today_table:
        if row.find("span", class_="today"): # –ò—â–µ—Ç –º–∞—Ç—á–∏ –Ω–∞ —Å–µ–≥–æ–¥–Ω—è
            match_name = row.find('a', title="Click for match detail").text.replace("-", "üÜö")
            match_url = f"https://www.tennisexplorer.com{row.find('a', title='Click for match detail')['href']}"
            match_time = row.find("td", class_="first time").text

            player1, player2 = get_player_info(match_url, headers=headers)

            match_info = {
                "m_name": match_name,
                "m_time": match_time,
                "m_url": match_url,
                "p1_full_name": player1.get("name"),
                "p1_url": player1.get("url"),
                "p2_full_name": player2.get("name"),
                "p2_url": player2.get("url")
            }

            today_matches.append(match_info)

    return today_matches

def update_matches_data(url_list: list, headers:dict):
    """
    :param url_list: –°–ø–∏—Å–æ–∫ URL
    :param headers:
    :return: –ü–∏—à–µ—Ç json —Ñ–∞–π–ª—ã –ø–æ —Ç—É—Ä–Ω–∏—Ä–∞–º –Ω–∞ —Å–µ–≥–æ–¥–Ω—è—à–Ω–∏–π –¥–µ–Ω—å
    –í–Ω—É—Ç—Ä–∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö, –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑ url –Ω–∞–∑–≤–∞–Ω–∏—è —Ç—É—Ä–Ω–∏—Ä–æ–≤ –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –ø–æ–¥ –∏—Ö –∏–º–µ–Ω–∞–º–∏ json
    """
    for url in url_list:
        tennis_tour = re.search(r'([^/]+)/?$', url).group(1).replace("-", "_")
        print(tennis_tour)
        today_matches = get_matches_today(url=url, headers=headers)

        with open(f"data/{tennis_tour}today.json", "w", encoding="utf-8") as file:
            json.dump(today_matches, file, ensure_ascii=False, indent=4)



update_matches_data(url_list=url_list, headers=headers)